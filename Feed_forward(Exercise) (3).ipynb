{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gautam/Downloads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "%cd /home/gautam/Downloads\n",
    "df = pd.read_csv('Assignment3_train_data.csv')\n",
    "X = df.loc[1:,'feature0':'feature783'].to_numpy()/255\n",
    "Y_temp = df['label'].as_matrix()\n",
    "Y = np.zeros([len(Y_temp),4])\n",
    "for i in range(len(Y_temp)):\n",
    "    temp = [0,0,0,0]\n",
    "    temp[Y_temp[i]-1] = 1\n",
    "    Y[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network():\n",
    "    def __init__(self, neurons, hidden_acti, output_acti): # arguments: an array \"neurons\" consist of number of neurons for each layer, activation function to be used in hidden layers and activation function to be used in output layer\n",
    "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
    "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
    "        self.layers = len(neurons)\n",
    "        self.w = []\n",
    "        for i in range(len(neurons)-1): \n",
    "            self.w.append(np.random.uniform(1e-6,1e-2,(neurons[i],neurons[i+1]))) #weight matrix between layer i and layer i+1\n",
    "        self.activationHidden = None # Activation funtion to be used in hidden layers\n",
    "        self.activationOutput = None # Activation funtion to be used in output layer\n",
    "        self.activationHiddenPrime = None # Derivative of the activation funtion to be used in hidden layers\n",
    "        self.activationOutputPrime = None # Derivative of the activation funtion to be used in output layer\n",
    "        \n",
    "        if(hidden_acti == \"sigmoid\"):\n",
    "            self.activationHidden = self.sigmoid\n",
    "            self.activationHiddenPrime = self.sigmoidPrime\n",
    "        else:\n",
    "            self.activationHidden = self.linear\n",
    "            self.activationHiddenPrime = self.linearPrime\n",
    "            \n",
    "        if(output_acti == \"sigmoid\"):\n",
    "            self.activationOutput = self.sigmoid\n",
    "            self.activationOutputPrime = self.sigmoidPrime\n",
    "        else:\n",
    "            self.activationOutput = self.linear\n",
    "            self.activationOutputPrime = self.linearPrime\n",
    "            \n",
    "    def sigmoid(self, s): # sigmoid activation function\n",
    "        return(1/(1+np.exp(-s)))\n",
    "    \n",
    "    def sigmoidPrime(self,x): # derivative sigmoid activation function\n",
    "        return(self.sigmoid(x)*(1-self.sigmoid(x)))\n",
    "    \n",
    "    def linear(self, s): # Linear activation function\n",
    "        return(s)\n",
    "    \n",
    "    def linearPrime(self,x): # derivative of linear activation function\n",
    "        return(np.ones(len(x)))\n",
    "    \n",
    "    def meanSquareError(self,y, y_calculated):\n",
    "        return((y_calculated - y)**2)\n",
    "    \n",
    "    def meanSquareErrorPrime(self,y, y_calculated):\n",
    "        return(2*(y_calculated - y))\n",
    "    \n",
    "    def crossEntropyLoss(self,y, y_calculated):\n",
    "        loss = y*np.log(y_calculated)\n",
    "        return(-1*sum(loss))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        # function of forward pass which will receive input and give the output of final layer\n",
    "        # Write the forward pass using the weights to find the predicted value and return it.\n",
    "        inputs = x\n",
    "        for i in range(len(self.w)-1):\n",
    "            z = inputs.dot(self.w[i])\n",
    "            activated = self.activationHidden(z)\n",
    "            inputs = activated\n",
    "        z = inputs.dot(self.w[-1])\n",
    "        y_calculated = self.activationOutput(z)\n",
    "        return y_calculated\n",
    "    \n",
    "    def backward(self, x, y): \n",
    "        # find the loss and return derivative of loss w.r.t every parameter\n",
    "        # Write the backpropagation algorithm here to find the gradients and return it.\n",
    "        # Stores the z for  all the layers\n",
    "        z_products = []\n",
    "        # Stores the inputs for all the layers\n",
    "        activations = [x]\n",
    "        inputs = x\n",
    "        for i in range(len(self.w)-1):\n",
    "            z = inputs.dot(self.w[i])\n",
    "            z_products.append(z)\n",
    "            a = self.activationHidden(z)\n",
    "            activations.append(a)\n",
    "            inputs = a\n",
    "        z = inputs.dot(self.w[-1])\n",
    "        z_products.append(z)\n",
    "        y_calculated = self.activationOutput(z)\n",
    "\n",
    "        z_products = np.array(z_products)\n",
    "\n",
    "        activations = np.array(activations)\n",
    "        \n",
    "        g_L = self.meanSquareErrorPrime(y, y_calculated)\n",
    "        gl = g_L\n",
    "        dw = [0]*(self.layers-1)\n",
    "        dE_dwL = np.outer(activations[-1],(g_L)*(self.activationOutputPrime(z_products[-1])))\n",
    "        dw[-1] = dE_dwL\n",
    "        \n",
    "        l = self.layers-2\n",
    "        while(l>0):\n",
    "            phi_prime_zl = self.activationHiddenPrime(z_products[l])\n",
    "            phi_prime_zl_1 = self.activationHiddenPrime(z_products[l-1])\n",
    "            #diag_phi_prime_zl = np.identity(len(phi_prime_zl)) * np.outer(np.ones(len(phi_prime_zl)), phi_prime_zl)\n",
    "            gl = np.matmul(self.w[l],(phi_prime_zl*gl))\n",
    "            dE_dzl = gl*(phi_prime_zl_1)\n",
    "            dE_dwl = np.outer(activations[l-1],dE_dzl)\n",
    "            dw[l-1] = dE_dwl # prepending the derivative\n",
    "            l -= 1\n",
    "        return dw\n",
    "        \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        # update the parameters using the gradients\n",
    "        # update each parameter using the gradients and the learning rate\n",
    "        for i in range(len(self.w)):\n",
    "            self.w[i] -= learning_rate*grads[i]\n",
    "                     \n",
    "    def train(self, X, Y):     # receive the full training data set\n",
    "        lr = 1e-3              # learning rate\n",
    "        epochs = 50            # number of epochs\n",
    "        mini_batch_size = 50   # the size of mini batch\n",
    "        n = len(X)\n",
    "        for e in range(epochs):\n",
    "            loss = []\n",
    "            mapIndexPosition = list(zip(X, Y))\n",
    "            random.shuffle(mapIndexPosition)\n",
    "            mini_batches = [mapIndexPosition[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches: \n",
    "                gradients = np.zeros_like(self.w)\n",
    "                for x,y in mini_batch:\n",
    "                    out = self.forward(x) # call of forward pass to get the predicted value\n",
    "                    los = self.meanSquareError(y, out)\n",
    "                    loss.append(sum(los))\n",
    "                    grads = self.backward(x, y) # find the gradients using backward pass\n",
    "                    gradients += np.array(grads)\n",
    "                self.update_parameters(gradients/mini_batch_size, lr)\n",
    "            print(e, sum(loss))\n",
    "#             plt.plot(e,sum(loss))\n",
    "#             plt.show()\n",
    "        \n",
    "    def predict(self,x):\n",
    "        print (\"Input : \\n\" + str(x))\n",
    "        print (\"Output: \\n\" + str((self.forward(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#D_in is input dimension\n",
    "# H1 is dimension of first hidden layer \n",
    "# H2 is dimension of first hidden layer\n",
    "#D_out is output dimension.\n",
    "D_in, H1, H2, H3, D_out = 784, 300, 500, 300, 4\n",
    "\n",
    "neurons = [D_in, H1, H2, H3, D_out] # list of number of neurons in the layers sequentially.\n",
    "\n",
    "Hidden_activation = \"sigmoid\"   # activation function of the hidden layers.\n",
    "Output_activation = \"linear\"  # activation function of the output layer.\n",
    "test = Neural_Network(neurons, Hidden_activation, Output_activation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18383.868160920807\n"
     ]
    }
   ],
   "source": [
    "test.train(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for a data point after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : \n",
      "[3 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (784,300) not aligned: 2 (dim 0) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c13838082f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ab24624d5d63>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Input : \\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Output: \\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ab24624d5d63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mactivated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (784,300) not aligned: 2 (dim 0) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "test.predict(np.array([3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 135\n",
    "print(Y_temp[i])\n",
    "plt.imshow(X[i].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[0].reshape(28,28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
